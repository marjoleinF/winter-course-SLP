---
title: "Weekly Exercise - Week 5"
author: "Marjolein Fokkema"
output: pdf_document
---

```{r, echo=FALSE, eval=FALSE}
masq <- read.table("masq3597.txt", header = TRUE)
masq <- masq[ , c("D_DEPDYS", "GENDER", "Leeftijd", 
                  paste0("DEMOG", 1:8),
                  paste0("MASQ0", 1:9), paste0("MASQ", 11:90))]
sapply(masq, function(x) table(is.na(x)))
sapply(masq, table)
for (j in paste0("DEMOG", 1:8)) {
  masq[ , j] <- ifelse(is.na(masq[,j]), "NA", masq[,j])
}
saveRDS(masq[1:(nrow(masq)/2),], file = "masq_train.Rda")
saveRDS(masq[((nrow(masq)/2)+1):nrow(masq),], file = "masq_test.Rda")
```

```{r}
train <- readRDS("masq_train.Rda")
test <- readRDS("masq_test.Rda")
```

We use a real dataset on prediction of depressive disorder for this week's exercise. Possible predictor variables are item scores on the Mood and Anxiety Symptom Questionnaire (`MASQ01` - `MASQ90`) and socio-demographic characteristics (`GENDER`, `Leeftijd`, `DEMOG1` - `DEMOG8`). The response variable is `D_DEPDYS`, whether the respondent has a current depressive or dysthymic disorder (0 = no, 1 = yes), as evaluated by a mental-health professional through a structured interview.

In this assignment, you will fit a range of penalized logistic regression models on the training dataset and compare their performance on a test dataset. You will do this using the cross-validation + test set approach.

* Pick three candidate procedures from ridge, elastic net (with any $0 \leq \alpha \leq 1$), lasso, relaxed lasso. Ideally, this should be done by considering the training set (e.g., multicollinearity), and/or thinking about what would constitute a useful result (e.g., would a (non)-sparse solution be useful for decision making in clinical practice?), but you can also just make a random choice. 

* Select the most accurate model through 10-fold cross-validation on the training set.

* Estimate the misclassification rate (MCR) and the Brier score on the test set. 

  * The Brier score is the mean squared error of predicted probabilities: $\frac{(y-\hat{p})^2}{n}$
  * By default, `predict.cv.glmnet` returns predicted values on the scale of the linear predictor ($\hat{\eta}$), so to obtain predicted probabilities, compute $\hat{p} = \frac{1}{1 + e^{-\hat{\eta}}}$. 

* Use the `coef` method to extract the selected variables and their coefficients from the best-performing model. From which MASQ subscale were most items selected?

  * Anhedonic Depression: Items 1, 14, 18, 21, 23, 26, 27, 30, 33, 35, 36, 39, 40, 44, 49, 53, 58, 66, 72, 78, 86 and 89.

  * Anxious Arousal: Items 3, 19, 25, 45, 48, 52, 55, 57, 61, 67, 69, 73, 75, 79, 85, 87 and 88.

  * General Distress Depression: Items 6, 8, 10, 13, 16, 22, 24, 42, 47, 56, 64 and 74.

  * General Distress Anxiety: Items 2, 9, 12, 15, 20, 59, 63, 65, 77, 81 and 82.

  * General Distress Mixed: Items 4, 5, 17, 29, 31, 34, 37, 50, 51, 70, 76, 80, 83, 84 and 90.


```{r, eval=FALSE, echo=FALSE}
library("glmnet")
x <- model.matrix(D_DEPDYS ~ ., data = train)
y <- train$D_DEPDYS
x_test <- model.matrix(D_DEPDYS ~ ., data = test)
y_test <- test$D_DEPDYS
set.seed(42)
rl_mod <- cv.glmnet(x = x, y = y, relax = TRUE, family = "binomial")
rl_mod
plot(rl_mod)

rl_preds <- predict(rl_mod, newx = x_test)
table(rl_preds > 0, y_test)
mean((probs - y_test)^2) ## Brier

coef(rl_mod)
```




---
title: "Answers to exercises Gradient Boosting"
output: pdf_document
---


# Exercise 1: Fit a boosted ensemble to the Head Start data

```{r, message=FALSE,warning=FALSE, fig.width=6, fig.height=4}
library("gbm")
```

Note that if you load the `gbm` package you'll get a message that you should consider using package `gbm3` instead, which is in fact no longer available, so you can ignore this message.

a) The learning rate is controlled by the `shrinkage` argument which defaults to a value of .1. The `n.trees` argument controls the number of generated trees, which defaults to 100. The `bag.fraction` argument controls the fraction of training set observations randomly generated to fit each tree in the ensemble and defaults to 0.5. Tree depth is controlled by argument `interaction.depth`, which defaults to 1 (trees with a single split and 2 terminal nodes, i.e., main effects only). 

b) 
 
```{r, message=FALSE,warning=FALSE, fig.width=6, fig.height=4}
HSdata <- readRDS(file = "HeadStart.Rda")
set.seed(42)
train_ids <- sample(1:nrow(HSdata), size = nrow(HSdata)/2)
boost.ens <- gbm(Test_Pct04 ~ ., data = HSdata[train_ids,], 
                 distribution = "gaussian")
```

c) 

```{r, message=FALSE,warning=FALSE, fig.width=5, fig.height=6}
par(mfrow = c(2, 1))
gbm.perf(boost.ens, oobag.curve = TRUE)
```

First plot: The black curve depicts training error as a function of the number of trees. In general, it can only decrease. The blue curve corresponds to the 2nd y axis on the right, reflecting the cumulative improvement in loss (so it is more or less the exact opposite of the black curve). The blue vertical dotted line gives the iteration where OOB error starts to deteriorate. The printed message (suppressed here) notes that `OOB generally underestimates the optimal number of iterations` so we should take this approximation with a grain of salt.

The second plot gives a curve for the improvement in loss from one iteration or tree to the next. The change is positive for the first trees, the vertical line is drawn at a change of 0, and the change becomes negative afterwards. This is where the location of the vertical line in the previous plot comes from. The thick black lines gives the empirical estimates, the red curve is a smoothened version of that.

From these plots, we can conclude that given the learning rate of .01, we have fitted enough trees. We might want to use 100 of the trees, or slightly less, for prediction.

Also see `?gbm.perf` for an explanation of what these plots show

d)

```{r, message=FALSE,warning=FALSE, fig.width=6, fig.height=4}
imps <- summary(boost.ens, cBars = 20, las = 2, cex.names = .5)
```

The most important predictors are `AgeAFQT`, `PermInc` and `Race_Child`. 

e) By default, function `relative.influence` is used for computing importances, which does not use permutation, permutation would be used if we specify `method = permutation.test.gbm`. Because gradient boosted ensembles do not really have OOB observations. Trees are fitted on samples of the training data, but because of the sequential nature of gradient boosting, observations not part of the sample still influence the tree, so are not really OOB anymore.

f)

```{r, message=FALSE,warning=FALSE, fig.width=3.5, fig.height=3.5}
plot(boost.ens, i.var = "AgeAFQT")
plot(boost.ens, i.var = "PermInc")
plot(boost.ens, i.var = "Race_Child")
```

The PDPs display a positive effect of the mother's intelligence test score `AgeAFQT` and of the averaged family income `PermInc`. Further, Hispanic children (`Race_Child = 1`) appear to score higher on the 2004 test than White (`Race_Child = 1`) and Black (`Race_Child = 2`) and White.  

g) 

```{r}
gbm_preds_train <- predict(boost.ens, newdata = HSdata[train_ids, ])
mean((gbm_preds_train - HSdata$Test_Pct04[train_ids])^2)
gbm_preds_test <- predict(boost.ens, newdata = HSdata[-train_ids, ])
mean((gbm_preds_test - HSdata$Test_Pct04[-train_ids])^2)
```

The gradient boosting ensemble outperforms the single tree as well as the bagging and random forest ensembles on test data.

Note also how the gradient boosting ensemble has much less overoptimism on training data. Small trees are much more regularized and tend to overfit much less than the large trees in bagging and random forests.

We could test whether using a number of trees closer to the optimal given by OOB error would improve performance (in practice, we should avoid using several numbers of trees and using test data to choose what works best):

```{r}
gbm_preds_test2 <- predict(boost.ens, newdata = HSdata[-train_ids, ], n.trees = 50)
mean((gbm_preds_test2 - HSdata$Test_Pct04[-train_ids])^2)
gbm_preds_test2 <- predict(boost.ens, newdata = HSdata[-train_ids, ], n.trees = 75)
mean((gbm_preds_test2 - HSdata$Test_Pct04[-train_ids])^2)
```
We obtain worse performance than with the full ensemble. The OOB error indeed seems to underestimate the optimal number of trees for prediction, it does not seem to provide a very useful estimate in this data problem. 


\newpage
# Exercise 2: Fit a gradient boosted ensemble to the QSAR data

Load the data into R, code the response as a numeric 0-1 coded variable and separate data into training and test sets:

```{r}
qsar <- readRDS("qsar.Rda")
set.seed(42)
train <- sample(1:nrow(qsar), size = 700)
test <- which(!(1:nrow(qsar) %in% train))
gbm_dat <- qsar
gbm_dat$class <- as.numeric(gbm_dat$class) - 1
```

Fit a gradient-boosted ensemble to the training data:

```{r}
set.seed(42)
qsar.boost <- gbm(class ~ ., distribution = "bernoulli", n.trees = 1000, 
                  interaction.depth = 3, shrinkage = 0.01, data = gbm_dat[train, ])
```

a) 

```{r, message=FALSE,warning=FALSE, fig.width=5, fig.height=6}
par(mfrow = c(2, 1))
gbm.perf(qsar.boost, oobag.curve = TRUE)
```

OOB error decreases during the first $\approx$ 400 iterations and starts to increase somewhat after. Training error only decreases durting the 1,000 iterations.  


b) 

```{r, message=FALSE,warning=FALSE, fig.width=6, fig.height=4}
imps <- summary(qsar.boost, cBars = 20, las = 2, cex.names = .5)
```

As in the random forest fitted in the previous session, the most important predictors are `SpMax_Bm`, `SpPosA_B` and `SpMax_L`.

c) 

```{r, message=FALSE,warning=FALSE, fig.width=3.5, fig.height=3.5}
plot(qsar.boost, i.var = "SpMax_Bm")
plot(qsar.boost, i.var = "SpPosA_B")
plot(qsar.boost, i.var = "SpMax_L")
```

The effects of the three most important predictors appear negative and seem similar as to those in the random forest ensemble.

d) 

```{r}
qsar_boost_preds_train <- predict(qsar.boost, newdata = qsar[train, ])
qsar_boost_preds_test <- predict(qsar.boost, newdata = qsar[-train, ])
1 - sum(diag(prop.table(table(qsar_boost_preds_train > 0, qsar$class[train])))) ## train MCR
1 - sum(diag(prop.table(table(qsar_boost_preds_test > 0, qsar$class[-train])))) ## test MCR
```

The gradient boosted ensemble is outperformed by both the linear-kernel SVM and the random forest.

We could test whether using a number of trees closer to the optimum given by OOB error would improve performance (in practice, we should avoid using several numbers of trees and using test data to choose what works best):

```{r}
qsar_boost_preds_test2 <- predict(qsar.boost, newdata = qsar[-train, ], n.trees = 400)
1 - sum(diag(prop.table(table(qsar_boost_preds_test2 > 0, qsar$class[-train])))) ## test MCR
qsar_boost_preds_test3 <- predict(qsar.boost, newdata = qsar[-train, ], n.trees = 700)
1 - sum(diag(prop.table(table(qsar_boost_preds_test3 > 0, qsar$class[-train])))) ## test MCR
```

We obtain worse performance than with the full ensemble. The OOB error indeed seems to underestimate the optimal number of trees for prediction, it does not seem to provide a very useful estimate in this data problem. 







\newpage
# Exercise 3: Tune gradient boosting parameter

Load library, specify grid and prepare data:

```{r,warning=FALSE,message=FALSE}
library("caret")
grid <- expand.grid(shrinkage = c(.1, .01, .001),
                    n.trees = c(10, 100, 1000, 2000, 2500),
                    interaction.depth = 1:4,
                    n.minobsinnode = 10)
y <- HSdata$Test_Pct04
x <- HSdata[ , -which(names(HSdata) == "Test_Pct04")]
```

a) Apply function `train` (and wait patiently): 

```{r, eval=FALSE}
set.seed(42)
gbmFit <- train(x = x, y = y, tuneGrid = grid,
                distribution = "gaussian", method = "gbm", 
                trControl = trainControl(number = 10L, verboseIter = TRUE))
```

```{r,echo=FALSE}
#save(gbmFit, file = "gbmFit.Rda")
load("gbmFit.Rda")
```


b) Print and plot the result:

```{r, fig.width=7, fig.height=4}
#print(gbmFit) ## omitted for space considerations
plot(gbmFit)
gbmFit$bestTune
```

For `shrinkage` of .01 and .001, we obtain best performance with a tree depth of 1, suggesting main effects only and no interactions. Although tree depth of four seems to do better for a learning rate of .001, this might change if we add higher values for the number of trees (i.e., increase the number of boosting iterations). 

It is possible that more trees results in better (or even best) performance with shrinkage of .001, but I do not expect it to do much better than the current optimum at shrinkage of .01. 

For tree sizes higher than 1, we see degrading performance at shrinkage values of both .01 and .001. For the numbers of trees higher and lower than the optimal of 1,000, we see degrading performance at shrinkage value of .001. We could also try a somewhat finer grid for the number of trees in this range, but the plots do not suggest this will substantially improve performance. For this exercise, I think this set of parameter settings is good enough. 


c) For `interaction.depth`, a value of 1 seems optimal, higher values consistently yield worse accuracy. For `shrinkage = .001`, this pattern might also be observed if we try a larger number of trees.

Larger values of `n.trees` result in better, then in worse accuracy. For `shrinkage = .001`, this pattern might also be observed if we try a larger number of trees.

With a higher `shrinkage` of 0.1, the ensemble starts overfitting after 100 or more trees, especially with higher tree depth. 


d) Given the good performance of `interaction.depth` of 1 at `shrinkage` of .1 and .01, there do not appear to be (strong) interaction effects in the data.


e) Refit the ensemble using optimal parameter values: 

```{r, warning=FALSE,message=FALSE}
set.seed(42)
boost.ens2 <- gbm(Test_Pct04 ~ ., data = HSdata[train_ids, ], distribution = "gaussian",
             n.trees = 1000, interaction.depth=1, shrinkage = 0.01)
gbm_preds_test2 <- predict(boost.ens2, newdata = HSdata[-train_ids, ])
mean((gbm_preds_test2 - HSdata$Test_Pct04[-train_ids])^2)
```

The performance of the boosting model was not improved by tuning its parameters. Performance is even slightly worse, but very similar to that of the earlier boosting ensemble. Note that we could have expected this from the results of tuning: The default settings of shrinkage of .1, 100 trees and depth of 1 already performed very similar to the optimal settings.
